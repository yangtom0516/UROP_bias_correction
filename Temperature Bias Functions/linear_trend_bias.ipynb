{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a34466b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import deepsensor\n",
    "import deepsensor.torch\n",
    "from deepsensor.train import set_gpu_default_device\n",
    "set_gpu_default_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d2d43d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'.zmetadata'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/nfs/turbo/seas-dannes/urop-2024-bias/cfs-forecasts/cfs_2012-2023.zarr\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Open the Zarr store using Xarray\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m ds0 \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_zarr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidated\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Check the contents of the dataset\u001b[39;00m\n\u001b[1;32m     18\u001b[0m ds0\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xarray/backends/zarr.py:1535\u001b[0m, in \u001b[0;36mopen_zarr\u001b[0;34m(store, group, synchronizer, chunks, decode_cf, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, consolidated, overwrite_encoded_chunks, chunk_store, storage_options, decode_timedelta, use_cftime, zarr_version, zarr_format, use_zarr_fill_value_as_mask, chunked_array_type, from_array_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   1521\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   1522\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopen_zarr() got unexpected keyword arguments \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(kwargs\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m   1523\u001b[0m     )\n\u001b[1;32m   1525\u001b[0m backend_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1526\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msynchronizer\u001b[39m\u001b[38;5;124m\"\u001b[39m: synchronizer,\n\u001b[1;32m   1527\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconsolidated\u001b[39m\u001b[38;5;124m\"\u001b[39m: consolidated,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1532\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzarr_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: zarr_format,\n\u001b[1;32m   1533\u001b[0m }\n\u001b[0;32m-> 1535\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_cf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_cf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask_and_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_and_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_times\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconcat_characters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcat_characters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_coords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_coords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzarr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1545\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1546\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked_array_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked_array_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrom_array_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_array_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackend_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_timedelta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_timedelta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cftime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cftime\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1551\u001b[0m \u001b[43m    \u001b[49m\u001b[43mzarr_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzarr_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1552\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_zarr_fill_value_as_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_zarr_fill_value_as_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xarray/backends/api.py:687\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    675\u001b[0m decoders \u001b[38;5;241m=\u001b[39m _resolve_decoders_kwargs(\n\u001b[1;32m    676\u001b[0m     decode_cf,\n\u001b[1;32m    677\u001b[0m     open_backend_dataset_parameters\u001b[38;5;241m=\u001b[39mbackend\u001b[38;5;241m.\u001b[39mopen_dataset_parameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    683\u001b[0m     decode_coords\u001b[38;5;241m=\u001b[39mdecode_coords,\n\u001b[1;32m    684\u001b[0m )\n\u001b[1;32m    686\u001b[0m overwrite_encoded_chunks \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite_encoded_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 687\u001b[0m backend_ds \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdecoders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    693\u001b[0m ds \u001b[38;5;241m=\u001b[39m _dataset_from_backend_dataset(\n\u001b[1;32m    694\u001b[0m     backend_ds,\n\u001b[1;32m    695\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    706\u001b[0m )\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xarray/backends/zarr.py:1608\u001b[0m, in \u001b[0;36mZarrBackendEntrypoint.open_dataset\u001b[0;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, synchronizer, consolidated, chunk_store, storage_options, zarr_version, zarr_format, store, engine, use_zarr_fill_value_as_mask, cache_members)\u001b[0m\n\u001b[1;32m   1606\u001b[0m filename_or_obj \u001b[38;5;241m=\u001b[39m _normalize_path(filename_or_obj)\n\u001b[1;32m   1607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m store:\n\u001b[0;32m-> 1608\u001b[0m     store \u001b[38;5;241m=\u001b[39m \u001b[43mZarrStore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1612\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynchronizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynchronizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconsolidated\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconsolidated\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconsolidate_on_close\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunk_store\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_store\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzarr_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzarr_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1618\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_zarr_fill_value_as_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1619\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzarr_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzarr_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1620\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_members\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_members\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1621\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1623\u001b[0m store_entrypoint \u001b[38;5;241m=\u001b[39m StoreBackendEntrypoint()\n\u001b[1;32m   1624\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m close_on_error(store):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xarray/backends/zarr.py:732\u001b[0m, in \u001b[0;36mZarrStore.open_group\u001b[0;34m(cls, store, mode, synchronizer, group, consolidated, consolidate_on_close, chunk_store, storage_options, append_dim, write_region, safe_chunks, zarr_version, zarr_format, use_zarr_fill_value_as_mask, write_empty, cache_members)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mopen_group\u001b[39m(\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    725\u001b[0m     cache_members: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    726\u001b[0m ):\n\u001b[1;32m    727\u001b[0m     (\n\u001b[1;32m    728\u001b[0m         zarr_group,\n\u001b[1;32m    729\u001b[0m         consolidate_on_close,\n\u001b[1;32m    730\u001b[0m         close_store_on_close,\n\u001b[1;32m    731\u001b[0m         use_zarr_fill_value_as_mask,\n\u001b[0;32m--> 732\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[43m_get_open_params\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynchronizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynchronizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    736\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconsolidated\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconsolidated\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconsolidate_on_close\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconsolidate_on_close\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunk_store\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_store\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzarr_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzarr_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_zarr_fill_value_as_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_zarr_fill_value_as_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    743\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzarr_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzarr_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    746\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[1;32m    747\u001b[0m         zarr_group,\n\u001b[1;32m    748\u001b[0m         mode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    756\u001b[0m         cache_members,\n\u001b[1;32m    757\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xarray/backends/zarr.py:1807\u001b[0m, in \u001b[0;36m_get_open_params\u001b[0;34m(store, mode, synchronizer, group, consolidated, consolidate_on_close, chunk_store, storage_options, zarr_version, use_zarr_fill_value_as_mask, zarr_format)\u001b[0m\n\u001b[1;32m   1803\u001b[0m group \u001b[38;5;241m=\u001b[39m open_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m consolidated:\n\u001b[1;32m   1806\u001b[0m     \u001b[38;5;66;03m# TODO: an option to pass the metadata_key keyword\u001b[39;00m\n\u001b[0;32m-> 1807\u001b[0m     zarr_root_group \u001b[38;5;241m=\u001b[39m \u001b[43mzarr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_consolidated\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1808\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m consolidated \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1809\u001b[0m     \u001b[38;5;66;03m# same but with more error handling in case no consolidated metadata found\u001b[39;00m\n\u001b[1;32m   1810\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/zarr/convenience.py:1360\u001b[0m, in \u001b[0;36mopen_consolidated\u001b[0;34m(store, metadata_key, mode, **kwargs)\u001b[0m\n\u001b[1;32m   1357\u001b[0m         metadata_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta/root/consolidated/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m metadata_key\n\u001b[1;32m   1359\u001b[0m \u001b[38;5;66;03m# setup metadata store\u001b[39;00m\n\u001b[0;32m-> 1360\u001b[0m meta_store \u001b[38;5;241m=\u001b[39m \u001b[43mConsolidatedStoreClass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1362\u001b[0m \u001b[38;5;66;03m# pass through\u001b[39;00m\n\u001b[1;32m   1363\u001b[0m chunk_store \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk_store\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m store\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/zarr/storage.py:3046\u001b[0m, in \u001b[0;36mConsolidatedMetadataStore.__init__\u001b[0;34m(self, store, metadata_key)\u001b[0m\n\u001b[1;32m   3043\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstore \u001b[38;5;241m=\u001b[39m Store\u001b[38;5;241m.\u001b[39m_ensure_store(store)\n\u001b[1;32m   3045\u001b[0m \u001b[38;5;66;03m# retrieve consolidated metadata\u001b[39;00m\n\u001b[0;32m-> 3046\u001b[0m meta \u001b[38;5;241m=\u001b[39m json_loads(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmetadata_key\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m   3048\u001b[0m \u001b[38;5;66;03m# check format of consolidated metadata\u001b[39;00m\n\u001b[1;32m   3049\u001b[0m consolidated_format \u001b[38;5;241m=\u001b[39m meta\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzarr_consolidated_format\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/zarr/storage.py:1120\u001b[0m, in \u001b[0;36mDirectoryStore.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fromfile(filepath)\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: '.zmetadata'"
     ]
    }
   ],
   "source": [
    "from deepsensor.train import Trainer\n",
    "from deepsensor.model import ConvNP\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from deepsensor.data import DataProcessor, TaskLoader\n",
    "from tqdm import tqdm\n",
    "from scipy.special import sph_harm\n",
    "\n",
    "# Load first three files from dataset\n",
    "data_path = '/nfs/turbo/seas-dannes/urop-2024-bias/cfs-forecasts/cfs_2012-2023.zarr'\n",
    "\n",
    "# Open the Zarr store using Xarray\n",
    "ds0 = xr.open_zarr(data_path, consolidated=True)\n",
    "\n",
    "# Check the contents of the dataset\n",
    "ds0\n",
    "#ds = ds.sel(time=ds.time.dt.hour == 0)\n",
    "\n",
    "#ds = ds.sel(time=slice(\"2012-01-01\", \"2015-12-31\"))  # Select only the first three files\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5f913d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds0.isel(lead=0, drop=True)\n",
    "ds = ds.sortby(\"time\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d574b8dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c5496b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cc934b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829de108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply bias correction function\n",
    "\n",
    "def linear_trend_bias(data, slope=0.05):\n",
    "    return data + slope * np.arange(data.shape[0])[:, None, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8709d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds.sel(time=slice(\"2012-01-01T00:00:00.000000000\", \"2012-01-03T00:00:00.000000000\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36956ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processor = DataProcessor(x1_name=\"latitude\", x2_name=\"longitude\")\n",
    "_ = data_processor(ds.sel(time=slice(\"2012-01-01T00:00:00.000000000\", \"2012-01-31T00:00:00.000000000\")))\n",
    "ds_processed = data_processor(ds)\n",
    "print(data_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bba4839",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892beddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_range = (\"2012-01-01\", \"2014-12-31\")\n",
    "val_range = (\"2015-01-01\", \"2015-12-31\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6a7426",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6bbcb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bc9216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bias_function(data, bias_function, **kwargs):\n",
    "    return data + bias_function(data, **kwargs)\n",
    "\n",
    "# Define TaskLoader\n",
    "biased_contexts = [\n",
    "\n",
    "    add_bias_function(ds_processed[\"TMP_2maboveground\"], linear_trend_bias),\n",
    "    #add_bias_function(ds_processed[\"TMP_2maboveground\"], periodic_step_bias),\n",
    "]\n",
    "\n",
    "task_loader = TaskLoader(\n",
    "    context=biased_contexts,  \n",
    "    target=ds_processed[\"TMP_2maboveground\"],\n",
    ")\n",
    "\n",
    "print(task_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7efb29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "def gen_tasks(dates, progress=True):\n",
    "    tasks = []\n",
    "    for date in tqdm(dates):\n",
    "        date = np.datetime64(date)  # Ensure consistent datetime format\n",
    "        try:\n",
    "            task = task_loader(date, context_sampling=\"all\", target_sampling=\"all\")  \n",
    "            tasks.append(task)\n",
    "        except KeyError:\n",
    "            print(f\"Skipping date {date} as it is not found in dataset.\")\n",
    "        sys.stdout.flush()\n",
    "    print(f\"Finished generating {len(tasks)} tasks.\")\n",
    "    return tasks\n",
    "\n",
    "\n",
    "# Define the ConvNP model\n",
    "model = ConvNP(data_processor, task_loader, dim_yc=(1,))\n",
    "\n",
    "# Train the model\n",
    "trainer = Trainer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f4c98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"time\"] = pd.to_datetime(ds[\"time\"].values).strftime(\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e46358",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds.time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b00d9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "val_rmses = []\n",
    "train_rmses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6640f58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_val_rmse(model, val_tasks):\n",
    "    errors = []\n",
    "    target_var_ID = task_loader.target_var_IDs[0][0]  # assume 1st target set and 1D\n",
    "    for task in val_tasks:\n",
    "        mean = data_processor.map_array(model.mean(task), target_var_ID, unnorm=True)\n",
    "        true = data_processor.map_array(task[\"Y_t\"][0], target_var_ID, unnorm=True)\n",
    "        errors.extend(np.abs(mean - true))\n",
    "    return np.sqrt(np.mean(np.concatenate(errors) ** 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7286cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_train_rmse(model, train_tasks):\n",
    "    errors = []\n",
    "    context_var_ID = task_loader.context_var_IDs[0][0]  # assume 1st target set and 1D\n",
    "    for task in train_tasks:\n",
    "        mean = data_processor.map_array(model.mean(task), context_var_ID, unnorm=True)\n",
    "        true = data_processor.map_array(task[\"Y_t\"][0], context_var_ID, unnorm=True)\n",
    "        errors.extend(np.abs(mean - true))\n",
    "    return np.sqrt(np.mean(np.concatenate(errors) ** 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532de9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dates = pd.date_range(val_range[0], val_range[1])\n",
    "val_tasks = gen_tasks(val_dates)\n",
    "_ = model(val_tasks[0])\n",
    "print(f\"Model has {deepsensor.backend.nps.num_params(model.model):,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3c0bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "#deepsensor_folder = \"/home/whruiray/deepsensor_config/\"\n",
    "\n",
    "\n",
    "val_rmse_best = np.inf\n",
    "train_rmse_best = np.inf\n",
    "\n",
    "\n",
    "trainer = Trainer(model, lr=5e-5)\n",
    "train_tasks = gen_tasks(pd.date_range(train_range[0], train_range[1]), progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70f278c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(10)):\n",
    "    #train_tasks = gen_tasks(pd.date_range(train_range[0], train_range[1]), progress=False)\n",
    "    \n",
    "    print(f\"Training batch for Epoch {epoch+1} started...\")\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    batch_losses = trainer(train_tasks)\n",
    "    \n",
    "    \n",
    "    losses.append(np.mean(batch_losses))\n",
    "    print(f\"Training batch for Epoch {epoch+1} complete...\")\n",
    "    sys.stdout.flush()\n",
    "   \n",
    "    train_rmses.append(compute_train_rmse(model, train_tasks))\n",
    "    print(f\"Epoch {epoch+1} - Loss: {losses[-1]:.4f}, Validation RMSE: {train_rmses[-1]:.4f}\")\n",
    "    print(f\"Epoch {epoch+1} completed in {time.time()} seconds.\")\n",
    "    sys.stdout.flush()\n",
    "    if train_rmses[-1] < train_rmse_best:\n",
    "        train_rmse_best = train_rmses[-1]\n",
    "\n",
    "    val_rmses.append(compute_val_rmse(model, val_tasks))\n",
    "    print(f\"Epoch {epoch+1} - Loss: {losses[-1]:.4f}, Validation RMSE: {val_rmses[-1]:.4f}\")\n",
    "    print(f\"Epoch {epoch+1} completed in {time.time()} seconds.\")\n",
    "    sys.stdout.flush()\n",
    "    if val_rmses[-1] < val_rmse_best:\n",
    "        val_rmse_best = val_rmses[-1]\n",
    "\n",
    "        #model.save(deepsensor_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4a3f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create folder if it doesn't exist\n",
    "os.makedirs(\"fig\", exist_ok=True)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "axes[0].plot(losses)\n",
    "axes[1].plot(val_rmses)\n",
    "_ = axes[0].set_xlabel(\"Epoch\")\n",
    "_ = axes[1].set_xlabel(\"Epoch\")\n",
    "_ = axes[0].set_title(\"Training loss\")\n",
    "_ = axes[1].set_title(\"Validation RMSE\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"fig/linear_trend_bias_loss_and_rmse.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb4462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training metrics with adjusted scales\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Smooth the loss values (using simple moving average)\n",
    "window_size = 5\n",
    "smoothed_losses = np.convolve(losses, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "# Plot training loss\n",
    "ax1.plot(range(len(smoothed_losses)), smoothed_losses, 'b-', label='Training Loss')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Loss', color='b')\n",
    "ax1.tick_params('y', colors='b')\n",
    "\n",
    "# Create a second y-axis for validation RMSE\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(range(len(val_rmses)), val_rmses, 'r-', label='Validation RMSE')\n",
    "ax2.set_ylabel('RMSE', color='r')\n",
    "ax2.tick_params('y', colors='r')\n",
    "\n",
    "# Adjust scales to show downward trend\n",
    "if len(losses) > 0:\n",
    "    loss_min, loss_max = min(smoothed_losses), max(smoothed_losses)\n",
    "    ax1.set_ylim(loss_min - 0.1*(loss_max-loss_min), loss_max + 0.1*(loss_max-loss_min))\n",
    "\n",
    "if len(val_rmses) > 0:\n",
    "    rmse_min, rmse_max = min(val_rmses), max(val_rmses)\n",
    "    ax2.set_ylim(rmse_min - 0.1*(rmse_max-rmse_min), rmse_max + 0.1*(rmse_max-rmse_min))\n",
    "\n",
    "plt.title('Training Loss and Validation RMSE')\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"fig/linear_trend_bias_metrics.png\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6c2ba0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import deepsensor\n",
    "from deepsensor import plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select a sample date for visualization\n",
    "sample_date = \"2012-01-01\"\n",
    "sample_task = task_loader(sample_date, context_sampling=\"all\", target_sampling=\"all\")\n",
    "\n",
    "task_loader_linear_trend_bias = TaskLoader(\n",
    "    context=biased_contexts,\n",
    "    target=ds_processed[\"TMP_2maboveground\"],\n",
    ")\n",
    "# Plot context (biased) and target (original) data\n",
    "fid = deepsensor.plot.task(sample_task, task_loader=task_loader_linear_trend_bias)\n",
    "fig.savefig(\"fig/linear_trend_sample_context_target_plot.png\")\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caa366e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Choose the target date for prediction\n",
    "target_date = \"2016-01-15\"  # Example date in the 2016-2020 range\n",
    "\n",
    "# Step 2: Load the task for the target date (context and target data)\n",
    "task = task_loader(target_date, context_sampling=\"all\", target_sampling=\"all\")\n",
    "\n",
    "# Step 3: Run the model to get predictions\n",
    "# Note that X_t is just passed as ds. It gets the xarray structure from ds. \n",
    "pred_val = model.predict(task, X_t=ds)  # Just pass the task object\n",
    "\n",
    "# Step 4: Extract the relevant variable for predictions (e.g., 'APCP_surface' or 'TMP_2maboveground')\n",
    "# Assuming 'TMP_2maboveground' is the variable you're predicting\n",
    "predxr = pred_val['TMP_2maboveground']\n",
    "\n",
    "# Step 5: Plot the mean prediction\n",
    "predxr['mean'].plot(cmap='viridis')  # Plot mean prediction\n",
    "plt.title(f\"Prediction for {target_date}\")\n",
    "plt.savefig(f\"fig/prediction_for_linear_{target_date}_mean.png\")\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# Step 6: Plot the standard deviation as well\n",
    "predxr['std'].plot(cmap='viridis')  # Plot standard deviation\n",
    "plt.title(f\"Standard Deviation for {target_date}\")\n",
    "plt.savefig(f\"fig/prediction_for_linear_{target_date}_std.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2956e343",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03040d0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5fa992",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdcbf5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a149343",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e284d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "deepsensor_env_gpu",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python (deepsensor_env_gpu)",
   "language": "python",
   "name": "deepsensor_env_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
