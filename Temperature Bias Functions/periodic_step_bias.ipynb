{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131885c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a34466b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import deepsensor\n",
    "import deepsensor.torch\n",
    "from deepsensor.train import set_gpu_default_device\n",
    "set_gpu_default_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d2d43d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepsensor.train import Trainer\n",
    "from deepsensor.model import ConvNP\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from deepsensor.data import DataProcessor, TaskLoader\n",
    "from tqdm import tqdm\n",
    "from scipy.special import sph_harm\n",
    "\n",
    "# Load first three files from dataset\n",
    "data_path = '/nfs/turbo/seas-dannes/urop-2024-bias/cfs-forecasts/cfs_2012-2023.zarr'\n",
    "ds0 = xr.open_zarr(data_path, consolidated=True)\n",
    "\n",
    "# Check the contents of the dataset\n",
    "ds = ds0.isel(lead=0, drop=True)\n",
    "ds = ds.sortby(\"time\")\n",
    "#ds = ds.sel(time=ds.time.dt.hour == 0)\n",
    "\n",
    "#ds = ds.sel(time=slice(\"2012-01-01\", \"2015-12-31\"))  # Select only the first three files\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee5f913d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2012-01-01T00:00:00.000000000' '2012-01-01T06:00:00.000000000'\n",
      " '2012-01-01T12:00:00.000000000' ... '2024-01-01T06:00:00.000000000'\n",
      " '2024-01-01T12:00:00.000000000' '2024-01-01T18:00:00.000000000']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d574b8dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10c5496b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2012-01-01T00:00:00.000000000' '2012-01-01T06:00:00.000000000'\n",
      " '2012-01-01T12:00:00.000000000' ... '2015-01-01T06:00:00.000000000'\n",
      " '2015-01-01T12:00:00.000000000' '2015-01-01T18:00:00.000000000']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41cc934b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 3GB\n",
      "Dimensions:            (time: 4206, latitude: 181, longitude: 360)\n",
      "Coordinates:\n",
      "  * latitude           (latitude) float64 1kB -90.0 -89.0 -88.0 ... 89.0 90.0\n",
      "  * longitude          (longitude) float64 3kB 0.0 1.0 2.0 ... 357.0 358.0 359.0\n",
      "  * time               (time) datetime64[ns] 34kB 2012-01-01 ... 2015-01-01T1...\n",
      "Data variables:\n",
      "    LHTFL_surface      (time, latitude, longitude) float32 1GB dask.array<chunksize=(1, 91, 180), meta=np.ndarray>\n",
      "    SHTFL_surface      (time, latitude, longitude) float32 1GB dask.array<chunksize=(1, 91, 180), meta=np.ndarray>\n",
      "    TMP_2maboveground  (time, latitude, longitude) float32 1GB dask.array<chunksize=(1, 91, 180), meta=np.ndarray>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "829de108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply bias correction function\n",
    "\n",
    "def periodic_step_bias(data, period=5, step=3):\n",
    "    time_factor = ((np.arange(data.shape[0]) // period) % 2).reshape(-1, 1, 1)\n",
    "    return data + step * time_factor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a8709d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 7MB\n",
      "Dimensions:            (time: 9, latitude: 181, longitude: 360)\n",
      "Coordinates:\n",
      "  * latitude           (latitude) float64 1kB -90.0 -89.0 -88.0 ... 89.0 90.0\n",
      "  * longitude          (longitude) float64 3kB 0.0 1.0 2.0 ... 357.0 358.0 359.0\n",
      "  * time               (time) datetime64[ns] 72B 2012-01-01 ... 2012-01-03\n",
      "Data variables:\n",
      "    LHTFL_surface      (time, latitude, longitude) float32 2MB dask.array<chunksize=(1, 91, 180), meta=np.ndarray>\n",
      "    SHTFL_surface      (time, latitude, longitude) float32 2MB dask.array<chunksize=(1, 91, 180), meta=np.ndarray>\n",
      "    TMP_2maboveground  (time, latitude, longitude) float32 2MB dask.array<chunksize=(1, 91, 180), meta=np.ndarray>\n"
     ]
    }
   ],
   "source": [
    "print(ds.sel(time=slice(\"2012-01-01T00:00:00.000000000\", \"2012-01-03T00:00:00.000000000\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d36956ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataProcessor with normalisation params:\n",
      "{'LHTFL_surface': {'method': 'mean_std',\n",
      "                   'params': {'mean': 66.60301208496094,\n",
      "                              'std': 65.60218048095703}},\n",
      " 'SHTFL_surface': {'method': 'mean_std',\n",
      "                   'params': {'mean': 6.118175506591797,\n",
      "                              'std': 31.839258193969727}},\n",
      " 'TMP_2maboveground': {'method': 'mean_std',\n",
      "                       'params': {'mean': 276.7368469238281,\n",
      "                                  'std': 20.317398071289062}},\n",
      " 'coords': {'time': {'name': 'time'},\n",
      "            'x1': {'map': (-90.0, 269.0), 'name': 'latitude'},\n",
      "            'x2': {'map': (0.0, 359.0), 'name': 'longitude'}}}\n"
     ]
    }
   ],
   "source": [
    "data_processor = DataProcessor(x1_name=\"latitude\", x2_name=\"longitude\")\n",
    "_ = data_processor(ds.sel(time=slice(\"2012-01-01T00:00:00.000000000\", \"2012-01-31T00:00:00.000000000\")))\n",
    "ds_processed = data_processor(ds)\n",
    "print(data_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bba4839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 3GB\n",
      "Dimensions:            (time: 4206, x1: 181, x2: 360)\n",
      "Coordinates:\n",
      "  * time               (time) datetime64[ns] 34kB 2012-01-01 ... 2015-01-01T1...\n",
      "  * x1                 (x1) float64 1kB 0.0 0.002786 0.005571 ... 0.4986 0.5014\n",
      "  * x2                 (x2) float64 3kB 0.0 0.002786 0.005571 ... 0.9972 1.0\n",
      "Data variables:\n",
      "    LHTFL_surface      (time, x1, x2) float32 1GB dask.array<chunksize=(1, 91, 180), meta=np.ndarray>\n",
      "    SHTFL_surface      (time, x1, x2) float32 1GB dask.array<chunksize=(1, 91, 180), meta=np.ndarray>\n",
      "    TMP_2maboveground  (time, x1, x2) float32 1GB dask.array<chunksize=(1, 91, 180), meta=np.ndarray>\n"
     ]
    }
   ],
   "source": [
    "print(ds_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "892beddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_range = (\"2012-01-01\", \"2014-12-31\")\n",
    "val_range = (\"2015-01-01\", \"2015-12-31\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6a7426",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6bbcb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5bc9216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaskLoader(1 context sets, 1 target sets)\n",
      "Context variable IDs: (('TMP_2maboveground',),)\n",
      "Target variable IDs: (('TMP_2maboveground',),)\n"
     ]
    }
   ],
   "source": [
    "def add_bias_function(data, bias_function, **kwargs):\n",
    "    return data + bias_function(data, **kwargs)\n",
    "\n",
    "# Define TaskLoader\n",
    "biased_contexts = [\n",
    "    #add_bias_function(ds_processed[\"TMP_2maboveground\"], spherical_harmonic_bias),\n",
    "    #add_bias_function(ds_processed[\"TMP_2maboveground\"], random_noise_bias),\n",
    "    #add_bias_function(ds_processed[\"TMP_2maboveground\"], linear_trend_bias),\n",
    "    add_bias_function(ds_processed[\"TMP_2maboveground\"], periodic_step_bias),\n",
    "]\n",
    "\n",
    "task_loader = TaskLoader(\n",
    "    context=biased_contexts,  \n",
    "    target=ds_processed[\"TMP_2maboveground\"],\n",
    ")\n",
    "\n",
    "print(task_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7efb29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_yt inferred from TaskLoader: 1\n",
      "dim_aux_t inferred from TaskLoader: 0\n",
      "internal_density inferred from TaskLoader: 359\n",
      "encoder_scales inferred from TaskLoader: [np.float32(0.0013927576)]\n",
      "decoder_scale inferred from TaskLoader: 0.002785515320334262\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "def gen_tasks(dates, progress=True):\n",
    "    tasks = []\n",
    "    for date in tqdm(dates):\n",
    "        date = np.datetime64(date)  # Ensure consistent datetime format\n",
    "        try:\n",
    "            task = task_loader(date, context_sampling=\"all\", target_sampling=\"all\")  \n",
    "            tasks.append(task)\n",
    "        except KeyError:\n",
    "            print(f\"Skipping date {date} as it is not found in dataset.\")\n",
    "        sys.stdout.flush()\n",
    "    print(f\"Finished generating {len(tasks)} tasks.\")\n",
    "    return tasks\n",
    "\n",
    "\n",
    "# Define the ConvNP model\n",
    "model = ConvNP(data_processor, task_loader, dim_yc=(1,))\n",
    "\n",
    "# Train the model\n",
    "trainer = Trainer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76f4c98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"time\"] = pd.to_datetime(ds[\"time\"].values).strftime(\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10e46358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'time' (time: 4206)> Size: 34kB\n",
      "array(['2012-01-01 00:00:00', '2012-01-01 06:00:00', '2012-01-01 12:00:00',\n",
      "       ..., '2015-01-01 06:00:00', '2015-01-01 12:00:00',\n",
      "       '2015-01-01 18:00:00'], dtype=object)\n",
      "Coordinates:\n",
      "  * time     (time) object 34kB '2012-01-01 00:00:00' ... '2015-01-01 18:00:00'\n"
     ]
    }
   ],
   "source": [
    "print(ds.time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b00d9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "val_rmses = []\n",
    "train_rmses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6640f58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_val_rmse(model, val_tasks):\n",
    "    errors = []\n",
    "    target_var_ID = task_loader.target_var_IDs[0][0]  # assume 1st target set and 1D\n",
    "    for task in val_tasks:\n",
    "        mean = data_processor.map_array(model.mean(task), target_var_ID, unnorm=True)\n",
    "        true = data_processor.map_array(task[\"Y_t\"][0], target_var_ID, unnorm=True)\n",
    "        errors.extend(np.abs(mean - true))\n",
    "    return np.sqrt(np.mean(np.concatenate(errors) ** 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee7286cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_train_rmse(model, train_tasks):\n",
    "    errors = []\n",
    "    context_var_ID = task_loader.context_var_IDs[0][0]  # assume 1st target set and 1D\n",
    "    for task in train_tasks:\n",
    "        mean = data_processor.map_array(model.mean(task), context_var_ID, unnorm=True)\n",
    "        true = data_processor.map_array(task[\"Y_t\"][0], context_var_ID, unnorm=True)\n",
    "        errors.extend(np.abs(mean - true))\n",
    "    return np.sqrt(np.mean(np.concatenate(errors) ** 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "532de9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/365 [00:00<02:39,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping date 2015-01-02T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-01-03T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-01-04T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-01-05T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-01-06T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-01-07T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-01-08T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-01-09T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-01-10T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-01-11T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-01-12T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-01-13T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-01-14T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-01-15T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-01-16T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-01-17T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-01-18T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-01-19T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-01-20T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-01-21T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-01-22T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-01-23T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-01-24T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-01-25T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-01-26T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-01-27T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-01-28T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-01-29T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-01-30T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-01-31T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-02-01T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-02-02T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-02-03T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-02-04T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-02-05T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-02-06T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-02-07T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-02-08T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-02-09T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-02-10T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-02-11T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-02-12T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-02-13T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-02-14T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-02-15T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-02-16T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-02-17T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-02-18T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-02-19T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-02-20T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-02-21T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-02-22T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-02-23T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-02-24T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-02-25T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-02-26T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-02-27T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-02-28T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-03-01T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-03-02T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-03-03T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-03-04T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-03-05T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-03-06T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-03-07T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-03-08T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-03-09T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-03-10T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-03-11T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-03-12T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-03-13T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-03-14T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-03-15T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-03-16T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-03-17T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-03-18T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-03-19T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-03-20T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-03-21T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-03-22T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-03-23T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-03-24T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-03-25T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-03-26T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-03-27T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-03-28T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-03-29T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-03-30T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-03-31T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-04-01T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-04-02T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-04-03T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-04-04T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-04-05T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-04-06T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-04-07T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-04-08T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-04-09T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-04-10T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-04-11T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-04-12T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-04-13T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-04-14T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-04-15T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-04-16T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-04-17T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-04-18T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-04-19T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-04-20T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-04-21T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-04-22T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-04-23T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-04-24T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-04-25T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-04-26T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-04-27T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-04-28T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-04-29T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-04-30T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-05-01T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-05-02T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-05-03T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-05-04T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-05-05T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-05-06T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-05-07T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-05-08T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-05-09T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-05-10T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-05-11T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-05-12T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-05-13T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-05-14T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-05-15T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-05-16T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-05-17T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-05-18T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-05-19T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-05-20T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-05-21T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-05-22T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-05-23T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-05-24T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-05-25T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-05-26T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-05-27T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-05-28T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-05-29T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-05-30T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-05-31T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-06-01T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-06-02T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-06-03T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-06-04T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-06-05T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-06-06T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-06-07T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-06-08T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-06-09T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-06-10T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-06-11T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-06-12T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-06-13T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-06-14T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-06-15T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-06-16T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-06-17T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-06-18T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-06-19T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-06-20T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-06-21T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-06-22T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-06-23T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-06-24T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-06-25T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-06-26T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-06-27T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-06-28T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-06-29T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-06-30T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-07-01T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-07-02T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-07-03T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-07-04T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-07-05T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-07-06T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-07-07T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-07-08T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-07-09T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-07-10T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-07-11T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-07-12T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-07-13T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-07-14T00:00:00.000000 as it is not found in dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 195/365 [00:00<00:00, 478.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping date 2015-07-15T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-07-16T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-07-17T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-07-18T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-07-19T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-07-20T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-07-21T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-07-22T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-07-23T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-07-24T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-07-25T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-07-26T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-07-27T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-07-28T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-07-29T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-07-30T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-07-31T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-08-01T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-08-02T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-08-03T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-08-04T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-08-05T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-08-06T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-08-07T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-08-08T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-08-09T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-08-10T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-08-11T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-08-12T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-08-13T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-08-14T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-08-15T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-08-16T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-08-17T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-08-18T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-08-19T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-08-20T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-08-21T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-08-22T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-08-23T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-08-24T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-08-25T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-08-26T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-08-27T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-08-28T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-08-29T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-08-30T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-08-31T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-09-01T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-09-02T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-09-03T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-09-04T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-09-05T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-09-06T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-09-07T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-09-08T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-09-09T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-09-10T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-09-11T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-09-12T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-09-13T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-09-14T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-09-15T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-09-16T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-09-17T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-09-18T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-09-19T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-09-20T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-09-21T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-09-22T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-09-23T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-09-24T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-09-25T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-09-26T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-09-27T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-09-28T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-09-29T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-09-30T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-10-01T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-10-02T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-10-03T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-10-04T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-10-05T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-10-06T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-10-07T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-10-08T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-10-09T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-10-10T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-10-11T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-10-12T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-10-13T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-10-14T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-10-15T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-10-16T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-10-17T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-10-18T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-10-19T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-10-20T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-10-21T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-10-22T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-10-23T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-10-24T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-10-25T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-10-26T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-10-27T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-10-28T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-10-29T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-10-30T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-10-31T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-11-01T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-11-02T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-11-03T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-11-04T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-11-05T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-11-06T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-11-07T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-11-08T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-11-09T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-11-10T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-11-11T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-11-12T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-11-13T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-11-14T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-11-15T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-11-16T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-11-17T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-11-18T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-11-19T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-11-20T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-11-21T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-11-22T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-11-23T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-11-24T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-11-25T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-11-26T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-11-27T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-11-28T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-11-29T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-11-30T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-12-01T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-12-02T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-12-03T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-12-04T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-12-05T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-12-06T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-12-07T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-12-08T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-12-09T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-12-10T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-12-11T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-12-12T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-12-13T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-12-14T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-12-15T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-12-16T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-12-17T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-12-18T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-12-19T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-12-20T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-12-21T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-12-22T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-12-23T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-12-24T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-12-25T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-12-26T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-12-27T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-12-28T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-12-29T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-12-30T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2015-12-31T00:00:00.000000 as it is not found in dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 365/365 [00:00<00:00, 584.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished generating 1 tasks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whruiray/deepsensor_gpu_2/lib/python3.10/site-packages/lab/types.py:178: DeprecationWarning: numpy.core is deprecated and has been renamed to numpy._core. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numerictypes.\n",
      "  for name in np.core.numerictypes.__all__ + [\"bool\"]:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 1,027,844 parameters\n"
     ]
    }
   ],
   "source": [
    "val_dates = pd.date_range(val_range[0], val_range[1])\n",
    "val_tasks = gen_tasks(val_dates)\n",
    "_ = model(val_tasks[0])\n",
    "print(f\"Model has {deepsensor.backend.nps.num_params(model.model):,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b3c0bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 60/1096 [00:23<06:52,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping date 2012-03-01T00:00:00.000000 as it is not found in dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 176/1096 [01:10<05:56,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping date 2012-06-25T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2012-06-26T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2012-06-27T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2012-06-28T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2012-06-29T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2012-06-30T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2012-07-01T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2012-07-02T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2012-07-03T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2012-07-04T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2012-07-05T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2012-07-06T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2012-07-07T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2012-07-08T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2012-07-09T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2012-07-10T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2012-07-11T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2012-07-12T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2012-07-13T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2012-07-14T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2012-07-15T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2012-07-16T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2012-07-17T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2012-07-18T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2012-07-19T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2012-07-20T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2012-07-21T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2012-07-22T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2012-07-23T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2012-07-24T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2012-07-25T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2012-07-26T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2012-07-27T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2012-07-28T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2012-07-29T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2012-07-30T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2012-07-31T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2012-08-01T00:00:00.000000 as it is not found in dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 215/1096 [01:10<00:27, 31.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping date 2012-08-05T00:00:00.000000 as it is not found in dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 277/1096 [01:34<05:13,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping date 2012-10-04T00:00:00.000000 as it is not found in dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 334/1096 [01:55<04:50,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping date 2012-11-30T00:00:00.000000 as it is not found in dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 501/1096 [02:59<03:53,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping date 2013-05-16T00:00:00.000000 as it is not found in dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▊     | 533/1096 [03:11<03:35,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping date 2013-06-17T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2013-06-18T00:00:00.000000 as it is not found in dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 1069/1096 [06:36<00:10,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping date 2014-12-05T00:00:00.000000 as it is not found in dataset.\n",
      "Skipping date 2014-12-06T00:00:00.000000 as it is not found in dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1096/1096 [06:46<00:00,  2.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished generating 1049 tasks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "#deepsensor_folder = \"/home/whruiray/deepsensor_config/\"\n",
    "\n",
    "\n",
    "val_rmse_best = np.inf\n",
    "train_rmse_best = np.inf\n",
    "\n",
    "\n",
    "trainer = Trainer(model, lr=5e-5)\n",
    "train_tasks = gen_tasks(pd.date_range(train_range[0], train_range[1]), progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70f278c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch for Epoch 1 started...\n",
      "Training batch for Epoch 1 complete...\n",
      "Epoch 1 - Loss: -0.8949, Validation RMSE: 1.8689\n",
      "Epoch 1 completed in 1744040768.6207387 seconds.\n",
      "Epoch 1 - Loss: -0.8949, Validation RMSE: 1.9150\n",
      "Epoch 1 completed in 1744040768.7215345 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [06:06<54:58, 366.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch for Epoch 2 started...\n",
      "Training batch for Epoch 2 complete...\n",
      "Epoch 2 - Loss: -1.1087, Validation RMSE: 1.7299\n",
      "Epoch 2 completed in 1744041135.9032671 seconds.\n",
      "Epoch 2 - Loss: -1.1087, Validation RMSE: 1.7196\n",
      "Epoch 2 completed in 1744041136.005336 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [12:13<48:55, 366.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch for Epoch 3 started...\n",
      "Training batch for Epoch 3 complete...\n",
      "Epoch 3 - Loss: -1.1684, Validation RMSE: 1.7154\n",
      "Epoch 3 completed in 1744041503.7912586 seconds.\n",
      "Epoch 3 - Loss: -1.1684, Validation RMSE: 1.7345\n",
      "Epoch 3 completed in 1744041503.893594 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [18:21<42:51, 367.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch for Epoch 4 started...\n",
      "Training batch for Epoch 4 complete...\n",
      "Epoch 4 - Loss: -1.2064, Validation RMSE: 1.7263\n",
      "Epoch 4 completed in 1744041871.8972657 seconds.\n",
      "Epoch 4 - Loss: -1.2064, Validation RMSE: 1.7282\n",
      "Epoch 4 completed in 1744041871.9996293 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [24:29<36:46, 367.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch for Epoch 5 started...\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(10)):\n",
    "    #train_tasks = gen_tasks(pd.date_range(train_range[0], train_range[1]), progress=False)\n",
    "    \n",
    "    print(f\"Training batch for Epoch {epoch+1} started...\")\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    batch_losses = trainer(train_tasks)\n",
    "    \n",
    "    \n",
    "    losses.append(np.mean(batch_losses))\n",
    "    print(f\"Training batch for Epoch {epoch+1} complete...\")\n",
    "    sys.stdout.flush()\n",
    "   \n",
    "    train_rmses.append(compute_train_rmse(model, train_tasks))\n",
    "    print(f\"Epoch {epoch+1} - Loss: {losses[-1]:.4f}, Validation RMSE: {train_rmses[-1]:.4f}\")\n",
    "    print(f\"Epoch {epoch+1} completed in {time.time()} seconds.\")\n",
    "    sys.stdout.flush()\n",
    "    if train_rmses[-1] < train_rmse_best:\n",
    "        train_rmse_best = train_rmses[-1]\n",
    "\n",
    "    val_rmses.append(compute_val_rmse(model, val_tasks))\n",
    "    print(f\"Epoch {epoch+1} - Loss: {losses[-1]:.4f}, Validation RMSE: {val_rmses[-1]:.4f}\")\n",
    "    print(f\"Epoch {epoch+1} completed in {time.time()} seconds.\")\n",
    "    sys.stdout.flush()\n",
    "    if val_rmses[-1] < val_rmse_best:\n",
    "        val_rmse_best = val_rmses[-1]\n",
    "\n",
    "        #model.save(deepsensor_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb4462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training metrics with adjusted scales\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Smooth the loss values (using simple moving average)\n",
    "window_size = 5\n",
    "smoothed_losses = np.convolve(losses, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "# Plot training loss\n",
    "ax1.plot(range(len(smoothed_losses)), smoothed_losses, 'b-', label='Training Loss')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Loss', color='b')\n",
    "ax1.tick_params('y', colors='b')\n",
    "\n",
    "# Create a second y-axis for validation RMSE\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(range(len(val_rmses)), val_rmses, 'r-', label='Validation RMSE')\n",
    "ax2.set_ylabel('RMSE', color='r')\n",
    "ax2.tick_params('y', colors='r')\n",
    "\n",
    "# Adjust scales to show downward trend\n",
    "if len(losses) > 0:\n",
    "    loss_min, loss_max = min(smoothed_losses), max(smoothed_losses)\n",
    "    ax1.set_ylim(loss_min - 0.1*(loss_max-loss_min), loss_max + 0.1*(loss_max-loss_min))\n",
    "\n",
    "if len(val_rmses) > 0:\n",
    "    rmse_min, rmse_max = min(val_rmses), max(val_rmses)\n",
    "    ax2.set_ylim(rmse_min - 0.1*(rmse_max-rmse_min), rmse_max + 0.1*(rmse_max-rmse_min))\n",
    "\n",
    "plt.title('Training Loss and Validation RMSE')\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4a3f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "axes[0].plot(losses)\n",
    "axes[1].plot(val_rmses)\n",
    "_ = axes[0].set_xlabel(\"Epoch\")\n",
    "_ = axes[1].set_xlabel(\"Epoch\")\n",
    "_ = axes[0].set_title(\"Training loss\")\n",
    "_ = axes[1].set_title(\"Validation RMSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6c2ba0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import deepsensor\n",
    "from deepsensor import plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select a sample date for visualization\n",
    "sample_date = \"2012-01-01\"\n",
    "sample_task = task_loader(sample_date, context_sampling=\"all\", target_sampling=\"all\")\n",
    "\n",
    "task_loader_periodic = TaskLoader(\n",
    "    context=biased_contexts,\n",
    "    target=ds_processed[\"TMP_2maboveground\"],\n",
    ")\n",
    "# Plot context (biased) and target (original) data\n",
    "deepsensor.plot.task(sample_task, task_loader=task_loader_periodic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caa366e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2956e343",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03040d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Choose the target date for prediction\n",
    "target_date = \"2016-01-15\"  # Example date in the 2016-2020 range\n",
    "\n",
    "# Step 2: Load the task for the target date (context and target data)\n",
    "task = task_loader(target_date, context_sampling=\"all\", target_sampling=\"all\")\n",
    "\n",
    "# Step 3: Run the model to get predictions\n",
    "# Note that X_t is just passed as ds. It gets the xarray structure from ds. \n",
    "pred_val = model.predict(task, X_t=ds)  # Just pass the task object\n",
    "\n",
    "# Step 4: Extract the relevant variable for predictions (e.g., 'APCP_surface' or 'TMP_2maboveground')\n",
    "# Assuming 'TMP_2maboveground' is the variable you're predicting\n",
    "predxr = pred_val['TMP_2maboveground']\n",
    "\n",
    "# Step 5: Plot the mean prediction\n",
    "predxr['mean'].plot(cmap='viridis')  # Plot mean prediction\n",
    "plt.title(f\"Prediction for {target_date}\")\n",
    "plt.show()\n",
    "\n",
    "# Step 6: Plot the standard deviation as well\n",
    "predxr['std'].plot(cmap='viridis')  # Plot standard deviation\n",
    "plt.title(f\"Standard Deviation for {target_date}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5fa992",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdcbf5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a149343",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e284d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deepsensor_gpu_2)",
   "language": "python",
   "name": "deepsensor_gpu_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
